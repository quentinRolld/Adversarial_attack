{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Maxout Network avec entraînement adversariale et évaluation sur base de teste adversariale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from maxout import CustomMaxout\n",
    "from adv_attack import create_adv_test\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize the images to [-1, 1]\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "batch_size = 64\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Défintion de la fonction de coût\n",
    "$$\\tilde{J}(\\theta, x, y) = \\alpha J(\\theta, x, y) + (1 - \\alpha) J(\\theta, x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)))$$\n",
    "\n",
    "On définit dans un premier temps une loss de base $$J(\\theta, x, y)$$ Comme rien n'est précisé dans l'article, on choisit la cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte standard\n",
    "def loss_fn(model, x, y):\n",
    "    output = model(x)\n",
    "    return F.cross_entropy(output, y)\n",
    "\n",
    "# Fonction de perte adversariale\n",
    "def adversarial_loss_fn(model, x, y, epsilon, alpha):\n",
    "    # Calcul de la perte standard\n",
    "    standard_loss = loss_fn(model, x, y)\n",
    "    \n",
    "    # Génération de l'exemple adverse\n",
    "    x_adv = x + epsilon * torch.sign(torch.autograd.grad(standard_loss, x, create_graph=True)[0])\n",
    "    \n",
    "    # Calcul de la perte sur l'exemple adverse\n",
    "    adversarial_loss = loss_fn(model, x_adv, y)\n",
    "    \n",
    "    # Combinaison des deux pertes\n",
    "    return alpha * standard_loss + (1 - alpha) * adversarial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Création des modèles utiles\n",
    "\n",
    "On a besoin d'un modèle  à 240 unit per layer et d'un autre  à 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 1\n",
    "dropout = 0.5\n",
    "\n",
    "#on créé le premier model qui à 240 unit per layer model\n",
    "Maxout_240U_Model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28*n_channels, 240),\n",
    "    CustomMaxout(240, 200, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(200, 160),\n",
    "    CustomMaxout(160, 120, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(120, 80),\n",
    "    CustomMaxout(80, 40, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(40, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "#on créé le second modèle qui a 1600 unit per layer\n",
    "Maxout_1600U_Model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28*n_channels, 1600),\n",
    "    CustomMaxout(1600, 1500, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(1500, 1400),\n",
    "    CustomMaxout(1400, 1300, n_channels, True),\n",
    "    nn.Linear(1300, 1200),\n",
    "    CustomMaxout(1200, 1100, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(1100, 1000),\n",
    "    CustomMaxout(1000, 800, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(800, 600),\n",
    "    CustomMaxout(600, 400, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(400, 200),  \n",
    "    CustomMaxout(200, 100, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(100, 50),\n",
    "    CustomMaxout(50, 25, n_channels, True),  \n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(25, 10),  \n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the early stopping variables\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_valid_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_epochs = 10  \n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "# Define the number of epochs\n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "# Define the training and validation data loaders\n",
    "train_dataloader = training_dataloader\n",
    "valid_dataloader = test_dataloader\n",
    "\n",
    "model_dict = {\"Maxout_240U_Model\": Maxout_240U_Model, \"Maxout_1600U_Model\": Maxout_1600U_Model}\n",
    "\n",
    "\n",
    "for model_name, model in model_dict.items(): \n",
    "   #Train the model\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs, labels = batch\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device).requires_grad_()\n",
    "            labels = labels.to(device).requires_grad_()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = adversarial_loss_fn(model, inputs, labels, epsilon, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train_preds += labels.size(0)\n",
    "            correct_train_preds += (predicted == labels).sum().item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_accuracies.append(100 * correct_train_preds / total_train_preds)\n",
    "        \n",
    "    # Plot the training losses and accuracies    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training loss', color=color)\n",
    "    ax1.plot(train_losses, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Training accuracy', color=color) \n",
    "    ax2.plot(train_accuracies, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    plt.show()\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct_valid_preds = 0\n",
    "    total_valid_preds = 0\n",
    "    for batch in valid_dataloader:\n",
    "        inputs, labels = batch\n",
    "        # Move the inputs and labels to the device\n",
    "        inputs = inputs.to(device).requires_grad_()\n",
    "        labels = labels.to(device).requires_grad_()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = adversarial_loss_fn(model, inputs, labels, epsilon, alpha)\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_valid_preds += labels.size(0)\n",
    "        correct_valid_preds += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_losses.append(valid_loss / len(valid_dataloader))\n",
    "    valid_accuracies.append(100 * correct_valid_preds / total_valid_preds)\n",
    "\n",
    "    #early stopping\n",
    "    # Check if the validation loss has improved\n",
    "    if valid_losses[-1] < best_valid_loss:\n",
    "        best_valid_loss = valid_losses[-1]\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # If the validation loss hasn't improved for early_stop_epochs, stop training\n",
    "    if epochs_no_improve == early_stop_epochs:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "        \n",
    "    print(f'Validation loss: {valid_losses[-1]:.3f}.. '\n",
    "          f'Validation accuracy: {valid_accuracies[-1]:.3f}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création set de test adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on récupère les images et les labels de la base de test\n",
    "x_test = torch.cat([images for images, labels in test_dataloader]).to(device) #images de la base test\n",
    "y_test = torch.cat([labels for images, labels in test_dataloader]).to(device) #labels de la base test\n",
    "\n",
    "eps = 0.1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#création des images adverses pour le réseau  à 240 unites\n",
    "altered_test_240U = create_adv_test(Maxout_240U_Model, x_test, y_test, eps, loss_func)\n",
    "\n",
    "#création des images adverses pour le réseau  à 1600 unites\n",
    "altered_test_1600U = create_adv_test(Maxout_1600U_Model, x_test, y_test, eps, loss_func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 . évaluation des modèles sur la nouvelle base de test adversariale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_preds = 0\n",
    "correct_test_preds = 0\n",
    "test_loss = 0\n",
    "misclassified_confidences = []\n",
    "for model_name, model in model_dict.items():\n",
    "    model.eval()  \n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device).requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        # Calculate adversarial loss and generate adversarial examples\n",
    "        loss = adversarial_loss_fn(model, inputs, labels, epsilon, alpha)\n",
    "        test_loss += loss.item()\n",
    "    \n",
    "        # Disable gradient calculation for prediction and accuracy calculation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.exp(outputs)  # Convert log probabilities to probabilities\n",
    "            _, predicted = torch.max(probabilities.data, 1)\n",
    "    \n",
    "        total_test_preds += labels.size(0)\n",
    "        correct_test_preds += (predicted == labels).sum().item()\n",
    "    \n",
    "        # Calculate confidence\n",
    "        confidence = torch.max(probabilities, dim=1)[0]\n",
    "    \n",
    "        # Store confidence of misclassified examples\n",
    "        misclassified = predicted != labels\n",
    "        misclassified_confidences.extend(confidence[misclassified].tolist())\n",
    "    \n",
    "    test_accuracy = 100 * correct_test_preds / total_test_preds\n",
    "    print(f'Test loss: {test_loss / len(test_dataloader):.3f}.. '\n",
    "          f'Test accuracy: {test_accuracy:.3f}')\n",
    "    print(f'Average confidence of misclassified examples: {np.mean(misclassified_confidences):.3f}\\n')\n",
    "    print(\"===============================================================================\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
