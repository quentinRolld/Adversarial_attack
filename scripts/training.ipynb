{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from maxout import CustomMaxout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize the images to [-1, 1]\n",
    "])\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "batch_size = 64\n",
    "\n",
    "training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modèle de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Défintion de la fonction de coût\n",
    "$$\\tilde{J}(\\theta, x, y) = \\alpha J(\\theta, x, y) + (1 - \\alpha) J(\\theta, x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)))$$\n",
    "\n",
    "On définit dans un premier temps une loss de base $$J(\\theta, x, y)$$. Comme rien n'est précisé dans l'article, on choisit la cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte standard\n",
    "def loss_fn(model, x, y):\n",
    "    output = model(x)\n",
    "    return F.cross_entropy(output, y)\n",
    "\n",
    "# Fonction de perte adversariale\n",
    "def adversarial_loss_fn(model, x, y, epsilon, alpha):\n",
    "    # Calcul de la perte standard\n",
    "    standard_loss = loss_fn(model, x, y)\n",
    "    \n",
    "    # Génération de l'exemple adverse\n",
    "    x_adv = x + epsilon * torch.sign(torch.autograd.grad(standard_loss, x, create_graph=True)[0])\n",
    "    \n",
    "    # Calcul de la perte sur l'exemple adverse\n",
    "    adversarial_loss = loss_fn(model, x_adv, y)\n",
    "    \n",
    "    # Combinaison des deux pertes\n",
    "    return alpha * standard_loss + (1 - alpha) * adversarial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Création des modèles utiles\n",
    "\n",
    "On a besoin d'un modèle  à 240 unit per layer et d'un autre  à 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des paramètres utiles à l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 chan in MNIST\n",
    "n_channels = 1\n",
    "dropout = 0.5\n",
    "\n",
    "#on créé le premier model qui à 240 unit per layer model\n",
    "Maxout_240U_Model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28*n_channels, 240),\n",
    "    CustomMaxout(240, 200, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(200, 160),\n",
    "    CustomMaxout(160, 120, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(120, 80),\n",
    "    CustomMaxout(80, 40, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(40, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "#on créé le second modèle qui a 1600 unit per layer\n",
    "Maxout_1600U_Model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28*n_channels, 1600),\n",
    "    CustomMaxout(1600, 1500, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(1500, 1400),\n",
    "    CustomMaxout(1400, 1300, n_channels, True),\n",
    "    nn.Linear(1300, 1200),\n",
    "    CustomMaxout(1200, 1100, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(1100, 1000),\n",
    "    CustomMaxout(1000, 800, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(800, 600),\n",
    "    CustomMaxout(600, 400, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(400, 200),  \n",
    "    CustomMaxout(200, 100, n_channels, True),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(100, 50),\n",
    "    CustomMaxout(50, 25, n_channels, True),  \n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(25, 10),  \n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainement du modèle avec une crossentropy et un optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the early stopping variables\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_valid_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_epochs = 20  \n",
    "\n",
    "# Define the model\n",
    "model_dict = {\"Maxout_240U_Model\": Maxout_240U_Model, \"Maxout_1600U_Model\": Maxout_1600U_Model}\n",
    "\n",
    "for model_name, model in model_dict.items():\n",
    "    # Define the optimizer\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the number of epochs\n",
    "    n_epochs = 100\n",
    "\n",
    "    # Define the training and validation data loaders\n",
    "    train_dataloader = training_dataloader\n",
    "    valid_dataloader = test_dataloader\n",
    "\n",
    "    # Train the model\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = loss_func(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "        valid_losses.append(valid_loss / len(valid_dataloader))\n",
    "        valid_accuracies.append(100 * correct_valid / total_valid)\n",
    "\n",
    "        #early stopping\n",
    "        # Check if the validation loss has improved\n",
    "        if valid_losses[-1] < best_valid_loss:\n",
    "            best_valid_loss = valid_losses[-1]\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # If the validation loss hasn't improved for early_stop_epochs, stop training\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}.. '\n",
    "              f'Train loss: {train_losses[-1]:.3f}.. '\n",
    "              f'Validation loss: {valid_losses[-1]:.3f}.. '\n",
    "              f'Train accuracy: {train_accuracies[-1]:.3f}.. '\n",
    "              f'Validation accuracy: {valid_accuracies[-1]:.3f}')\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(valid_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training accuracy')\n",
    "    plt.plot(valid_accuracies, label='Validation accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement avec adversarial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "\n",
    "for model_name, model in model_dict.items(): \n",
    "   #Train the model\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs, labels = batch\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device).requires_grad_()\n",
    "            labels = labels.to(device).requires_grad_()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = adversarial_loss_fn(model, inputs, labels, epsilon, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train_preds += labels.size(0)\n",
    "            correct_train_preds += (predicted == labels).sum().item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_dataloader))\n",
    "        train_accuracies.append(100 * correct_train_preds / total_train_preds)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct_valid_preds = 0\n",
    "        total_valid_preds = 0\n",
    "        for batch in valid_dataloader:\n",
    "            inputs, labels = batch\n",
    "            # Move the inputs and labels to the device\n",
    "            inputs = inputs.to(device).requires_grad_()\n",
    "            labels = labels.to(device).requires_grad_()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = adversarial_loss_fn(model, inputs, labels, epsilon, alpha)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_valid_preds += labels.size(0)\n",
    "            correct_valid_preds += (predicted == labels).sum().item()\n",
    "\n",
    "        valid_losses.append(valid_loss / len(valid_dataloader))\n",
    "        valid_accuracies.append(100 * correct_valid_preds / total_valid_preds)\n",
    "\n",
    "        #early stopping\n",
    "        # Check if the validation loss has improved\n",
    "        if valid_losses[-1] < best_valid_loss:\n",
    "            best_valid_loss = valid_losses[-1]\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # If the validation loss hasn't improved for early_stop_epochs, stop training\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs}.. '\n",
    "            f'Train loss: {train_losses[-1]:.3f}.. '\n",
    "            f'Train accuracy: {train_accuracies[-1]:.3f}.. '\n",
    "            f'Validation loss: {valid_losses[-1]:.3f}.. '\n",
    "            f'Validation accuracy: {valid_accuracies[-1]:.3f}')\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training loss')\n",
    "    plt.plot(valid_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training accuracy')\n",
    "    plt.plot(valid_accuracies, label='Validation accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
