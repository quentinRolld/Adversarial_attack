{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 11305704.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 53742543.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 11935618.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6149299.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modèle de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxoutCNN(\n",
      "  (architecture): Sequential(\n",
      "    (0): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): Maxout()\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (3): Dropout2d(p=0.4, inplace=False)\n",
      "    (4): Conv2d(6, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): Maxout()\n",
      "    (6): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (7): Dropout2d(p=0.4, inplace=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=800, out_features=240, bias=True)\n",
      "    (10): Maxout()\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=120, out_features=168, bias=True)\n",
      "    (13): Maxout()\n",
      "    (14): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from maxout import MaxoutCNN\n",
    "\n",
    "maxout_network = MaxoutCNN()\n",
    "print(maxout_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Défintion de la fonction de coût\n",
    "$$\\tilde{J}(\\theta, x, y) = \\alpha J(\\theta, x, y) + (1 - \\alpha) J(\\theta, x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y)))$$\n",
    "\n",
    "On définit dans un premier temps une loss de base $$J(\\theta, x, y)$$. Comme rien n'est précisé dans l'article, on choisit la cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte standard\n",
    "def loss_fn(model, x, y):\n",
    "    output = model(x)\n",
    "    return F.cross_entropy(output, y)\n",
    "\n",
    "# Fonction de perte adversariale\n",
    "def adversarial_loss_fn(model, x, y, epsilon, alpha):\n",
    "    # Calcul de la perte standard\n",
    "    standard_loss = loss_fn(model, x, y)\n",
    "    \n",
    "    # Génération de l'exemple adverse\n",
    "    x_adv = x + epsilon * torch.sign(torch.autograd.grad(standard_loss, x, create_graph=True)[0])\n",
    "    \n",
    "    # Calcul de la perte sur l'exemple adverse\n",
    "    adversarial_loss = loss_fn(model, x_adv, y)\n",
    "    \n",
    "    # Combinaison des deux pertes\n",
    "    return alpha * standard_loss + (1 - alpha) * adversarial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînemnt d'un CNN sans maxout \n",
    "\n",
    "On définit un CNN similaire avec à la place des maxout, des relu. On entraîne ce CNN sur la base de données MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (architecture): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "    (11): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (12): ReLU()\n",
      "    (13): Dropout(p=0.5, inplace=False)\n",
      "    (14): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.architecture = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5,padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.architecture(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "cnn_network = CNN()\n",
    "print(cnn_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrainement et Attaque sur le CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des paramètres utiles à l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "sgd= optim.SGD(cnn_network.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sylogue/Master 2/MLA/Adversarial_attack/scripts/training.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m outputs \u001b[39m=\u001b[39m cnn_network(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m adversarial_loss_fn(cnn_network, inputs, labels, epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m/home/sylogue/Master 2/MLA/Adversarial_attack/scripts/training.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m standard_loss \u001b[39m=\u001b[39m loss_fn(model, x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Génération de l'exemple adverse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x_adv \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m epsilon \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msign(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(standard_loss, x, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Calcul de la perte sur l'exemple adverse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/sylogue/Master%202/MLA/Adversarial_attack/scripts/training.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m adversarial_loss \u001b[39m=\u001b[39m loss_fn(model, x_adv, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loop over the dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "    # Loop over each batch of data\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs.requires_grad = True\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        sgd.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_network(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = adversarial_loss_fn(cnn_network, inputs, labels, epsilon=0.1, alpha=0.5)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
