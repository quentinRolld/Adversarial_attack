{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cite the article : RBF \"have low confidence when they are fooled\", \n",
    "# It has low precision but \"it does correctly respond by reducing its confidence considerably on points it does not 'understand'.\"\n",
    "# Result to reproduce : \n",
    "# Shallow RBF network with no hidden layers : \n",
    "#            - confidence on mistaken examples = 1.2%\n",
    "#            - confidence on clean test examples = 60.6%\n",
    "# We will use the MNSIT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importations\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import the functions from fgsm.py\n",
    "from fgsm_copie import adv_attack, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Print the dataset information\n",
    "#print(mnist.DESCR)\n",
    "\n",
    "# Access the features and target variables\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape :  (70000, 784)\n",
      "y1 shape :  (70000,)\n",
      "X1 type :  <class 'numpy.float32'>\n",
      "y1 type :  <class 'numpy.float32'>\n",
      "[5. 0. 4. ... 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "# We convert the data into numpy array (cause for now they are in pandas)\n",
    "X1 = np.array(X)\n",
    "y1 = np.array(y)\n",
    "\n",
    "# the labels y are string, we convert them into float32\n",
    "y1 = y1.astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"X1 shape : \", X1.shape)\n",
    "print(\"y1 shape : \", y1.shape)\n",
    "print(\"X1 type : \", type(X1[0][0]))\n",
    "print(\"y1 type : \", type(y1[0]))\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# There we create the class for our Shallow RBF network\n",
    "\n",
    "class ShallowRBF(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_centers):\n",
    "        super(ShallowRBF, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_centers, input_dim))\n",
    "        self.beta = nn.Parameter(torch.ones(num_centers))\n",
    "        self.fc = nn.Linear(num_centers, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the RBF activations\n",
    "        rbf_activations = torch.exp(-self.beta * torch.norm(x.unsqueeze(1) - self.centers, dim=2))\n",
    "\n",
    "        # Normalize the RBF activations\n",
    "        rbf_activations = rbf_activations / torch.sum(rbf_activations, dim=1, keepdim=True)\n",
    "\n",
    "        # Pass the normalized RBF activations through the linear layer\n",
    "        output = self.fc(rbf_activations)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "num_centers = 100\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "epsilon = 0.3\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/700], Loss: nan\n",
      "Epoch [1/10], Step [200/700], Loss: nan\n",
      "Epoch [1/10], Step [300/700], Loss: nan\n",
      "Epoch [1/10], Step [400/700], Loss: nan\n",
      "Epoch [1/10], Step [500/700], Loss: nan\n",
      "Epoch [1/10], Step [600/700], Loss: nan\n",
      "Epoch [1/10], Step [700/700], Loss: nan\n",
      "Epoch [2/10], Step [100/700], Loss: nan\n",
      "Epoch [2/10], Step [200/700], Loss: nan\n",
      "Epoch [2/10], Step [300/700], Loss: nan\n",
      "Epoch [2/10], Step [400/700], Loss: nan\n",
      "Epoch [2/10], Step [500/700], Loss: nan\n",
      "Epoch [2/10], Step [600/700], Loss: nan\n",
      "Epoch [2/10], Step [700/700], Loss: nan\n",
      "Epoch [3/10], Step [100/700], Loss: nan\n",
      "Epoch [3/10], Step [200/700], Loss: nan\n",
      "Epoch [3/10], Step [300/700], Loss: nan\n",
      "Epoch [3/10], Step [400/700], Loss: nan\n",
      "Epoch [3/10], Step [500/700], Loss: nan\n",
      "Epoch [3/10], Step [600/700], Loss: nan\n",
      "Epoch [3/10], Step [700/700], Loss: nan\n",
      "Epoch [4/10], Step [100/700], Loss: nan\n",
      "Epoch [4/10], Step [200/700], Loss: nan\n",
      "Epoch [4/10], Step [300/700], Loss: nan\n",
      "Epoch [4/10], Step [400/700], Loss: nan\n",
      "Epoch [4/10], Step [500/700], Loss: nan\n",
      "Epoch [4/10], Step [600/700], Loss: nan\n",
      "Epoch [4/10], Step [700/700], Loss: nan\n",
      "Epoch [5/10], Step [100/700], Loss: nan\n",
      "Epoch [5/10], Step [200/700], Loss: nan\n",
      "Epoch [5/10], Step [300/700], Loss: nan\n",
      "Epoch [5/10], Step [400/700], Loss: nan\n",
      "Epoch [5/10], Step [500/700], Loss: nan\n",
      "Epoch [5/10], Step [600/700], Loss: nan\n",
      "Epoch [5/10], Step [700/700], Loss: nan\n",
      "Epoch [6/10], Step [100/700], Loss: nan\n",
      "Epoch [6/10], Step [200/700], Loss: nan\n",
      "Epoch [6/10], Step [300/700], Loss: nan\n",
      "Epoch [6/10], Step [400/700], Loss: nan\n",
      "Epoch [6/10], Step [500/700], Loss: nan\n",
      "Epoch [6/10], Step [600/700], Loss: nan\n",
      "Epoch [6/10], Step [700/700], Loss: nan\n",
      "Epoch [7/10], Step [100/700], Loss: nan\n",
      "Epoch [7/10], Step [200/700], Loss: nan\n",
      "Epoch [7/10], Step [300/700], Loss: nan\n",
      "Epoch [7/10], Step [400/700], Loss: nan\n",
      "Epoch [7/10], Step [500/700], Loss: nan\n",
      "Epoch [7/10], Step [600/700], Loss: nan\n",
      "Epoch [7/10], Step [700/700], Loss: nan\n",
      "Epoch [8/10], Step [100/700], Loss: nan\n",
      "Epoch [8/10], Step [200/700], Loss: nan\n",
      "Epoch [8/10], Step [300/700], Loss: nan\n",
      "Epoch [8/10], Step [400/700], Loss: nan\n",
      "Epoch [8/10], Step [500/700], Loss: nan\n",
      "Epoch [8/10], Step [600/700], Loss: nan\n",
      "Epoch [8/10], Step [700/700], Loss: nan\n",
      "Epoch [9/10], Step [100/700], Loss: nan\n",
      "Epoch [9/10], Step [200/700], Loss: nan\n",
      "Epoch [9/10], Step [300/700], Loss: nan\n",
      "Epoch [9/10], Step [400/700], Loss: nan\n",
      "Epoch [9/10], Step [500/700], Loss: nan\n",
      "Epoch [9/10], Step [600/700], Loss: nan\n",
      "Epoch [9/10], Step [700/700], Loss: nan\n",
      "Epoch [10/10], Step [100/700], Loss: nan\n",
      "Epoch [10/10], Step [200/700], Loss: nan\n",
      "Epoch [10/10], Step [300/700], Loss: nan\n",
      "Epoch [10/10], Step [400/700], Loss: nan\n",
      "Epoch [10/10], Step [500/700], Loss: nan\n",
      "Epoch [10/10], Step [600/700], Loss: nan\n",
      "Epoch [10/10], Step [700/700], Loss: nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() missing 2 required positional arguments: 'normal_loader' and 'epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb Cellule 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Calculate the accuracy on the clean test set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m clean_acc \u001b[39m=\u001b[39m test(model, dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy on clean examples: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(clean_acc))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quentinrolland/Documents/GitHub/Adversarial_attack/RBF/RBF_vs_adversarial.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Calculate the accuracy on the adversarial test set\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: test() missing 2 required positional arguments: 'normal_loader' and 'epsilon'"
     ]
    }
   ],
   "source": [
    "\n",
    "# We will now train our Shallow RBF network\n",
    "\n",
    "# Initialize the model\n",
    "model = ShallowRBF(input_dim, num_classes, num_centers)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X = torch.from_numpy(X1).float()\n",
    "y = torch.squeeze(torch.from_numpy(y1).long())\n",
    "\n",
    "# Create the dataloader\n",
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the progress\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, batch_idx + 1, len(dataloader), loss.item()))\n",
    "            \n",
    "# We will now test our Shallow RBF network\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#test( model, device, normal_loader, epsilon )\n",
    "\n",
    "# Calculate the accuracy on the clean test set\n",
    "clean_acc = test(model, device, dataloader, epsilon)\n",
    "print('Accuracy on clean examples: {:.2f}%'.format(clean_acc))\n",
    "\n",
    "# Calculate the accuracy on the adversarial test set\n",
    "adversarial_acc = test(model, device, dataloader, epsilon)\n",
    "print('Accuracy on adversarial examples: {:.2f}%'.format(adversarial_acc))\n",
    "\n",
    "# Calculate the confidence of the model on the clean test set\n",
    "clean_confidence = test(model, dataloader, return_confidence=True)\n",
    "print('Average confidence on clean examples: {:.2f}%'.format(clean_confidence))\n",
    "\n",
    "# Calculate the confidence of the model on the adversarial test set\n",
    "adversarial_confidence = test(model, dataloader, adv_attack, return_confidence=True)\n",
    "print('Average confidence on adversarial examples: {:.2f}%'.format(adversarial_confidence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
