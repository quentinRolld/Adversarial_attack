{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_from_mnist, visualize_data\n",
    "from train import prep_data,create_model,training_loop,eval_test,eval_train, plot_losses,visualize_weights_and_signs\n",
    "from adv_attack import adv_attack,create_adv_test\n",
    "from RBF_architecture import ShallowRBF, RbfNet2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from ADV_generalize import adv_attack, test_Maxout\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from maxout import CustomMaxout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/var/folders/91/hvhdt2vj68l460f4fv_t0k0h0000gn/T/ipykernel_29621/3691792284.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['target'] = mnist.target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset shape: (20000, 785)\n",
      "Labels: 0        8\n",
      "1        4\n",
      "2        8\n",
      "3        7\n",
      "4        7\n",
      "        ..\n",
      "19995    8\n",
      "19996    3\n",
      "19997    1\n",
      "19998    7\n",
      "19999    6\n",
      "Name: target, Length: 20000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Convert labels to integers\n",
    "mnist.target = mnist.target.astype(int)\n",
    "\n",
    "data = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)\n",
    "data['target'] = mnist.target\n",
    "\n",
    "data = data.sample(n=20000, random_state=42)\n",
    "\n",
    "# Reset the index of the filtered dataset\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Print the shape of the filtered dataset\n",
    "print(\"Filtered dataset shape:\", data.shape)\n",
    "print(\"Labels:\", data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in target labels: 10\n"
     ]
    }
   ],
   "source": [
    "# Assuming y is your target labels\n",
    "y = data['target'].values\n",
    "\n",
    "# Normalize the data\n",
    "data_normalized = data.iloc[:, :-1].values / 255.0\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "num_classes = len(set(y))\n",
    "print(\"Unique classes in target labels:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On vire les classes autres que 4 pour le set de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([363])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/hvhdt2vj68l460f4fv_t0k0h0000gn/T/ipykernel_22715/966028133.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(np.delete(X_test_tensor, List_arg_not4, axis=0), dtype=torch.float32)\n",
      "/var/folders/91/hvhdt2vj68l460f4fv_t0k0h0000gn/T/ipykernel_22715/966028133.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(np.delete(y_test_tensor, List_arg_not4), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "List_arg_not4 = [i for i in range(len(y_test_tensor)) if y_test_tensor[i] != 4]\n",
    "\n",
    "X_test_tensor = torch.tensor(np.delete(X_test_tensor, List_arg_not4, axis=0), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.delete(y_test_tensor, List_arg_not4), dtype=torch.long)\n",
    "\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(X_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(X_test_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte standard\n",
    "def loss_fn(model, x, y):\n",
    "    output = model(x)\n",
    "    return F.cross_entropy(output, y)\n",
    "\n",
    "# Fonction de perte adversariale\n",
    "def adversarial_loss_fn(model, x, y, epsilon, alpha):\n",
    "    # Calcul de la perte standard\n",
    "    standard_loss = loss_fn(model, x, y)\n",
    "    \n",
    "    # Génération de l'exemple adverse\n",
    "    x_adv = x + epsilon * torch.sign(torch.autograd.grad(standard_loss, x, create_graph=True)[0])\n",
    "    \n",
    "    # Calcul de la perte sur l'exemple adverse\n",
    "    adversarial_loss = loss_fn(model, x_adv, y)\n",
    "    \n",
    "    # Combinaison des deux pertes\n",
    "    return alpha * standard_loss + (1 - alpha) * adversarial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model2, device, test_loader, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
