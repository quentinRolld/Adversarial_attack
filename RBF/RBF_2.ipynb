{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau inspiré de http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf\n",
    "\n",
    "(tiré du TP du cours CNN de Mr Bailly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # convertit les images [H,W,C] dont les valeurs sont dans [0,255] en tenseurs [C,H,W] dont les valeurs sont dans [0,1]\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "num_centers = 100\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "epsilon = 0.3\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# There we create the class for our Shallow RBF network\n",
    "\n",
    "class ShallowRBF(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_centers):\n",
    "        super(ShallowRBF, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_centers, input_dim))\n",
    "        self.beta = nn.Parameter(torch.ones(num_centers))\n",
    "        self.fc = nn.Linear(num_centers, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        # Calculate the RBF activations\n",
    "        rbf_activations = torch.exp(-self.beta * torch.norm(x.unsqueeze(1) - self.centers, dim=2))\n",
    "\n",
    "        # Normalize the RBF activations\n",
    "        rbf_activations = rbf_activations / torch.sum(rbf_activations, dim=1, keepdim=True)\n",
    "\n",
    "        # Pass the normalized RBF activations through the linear layer\n",
    "        output = self.fc(rbf_activations)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model1 = ShallowRBF(input_dim, num_classes, num_centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture de réseau inspirée de ce repository : https://github.com/insujeon/rbfn/blob/master/RBFN_MNIST.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RbfNet(nn.Module):\n",
    "    def __init__(self, centers, num_class=10):\n",
    "        super(RbfNet, self).__init__()\n",
    "        self.centers = centers\n",
    "        self.num_centers = centers.size(0)\n",
    "        self.num_class = num_class\n",
    "    \n",
    "        self.linear = torch.nn.Linear(self.num_centers, self.num_class, bias=True)\n",
    "        self.beta = nn.Parameter(torch.ones(1,self.num_centers)/10)\n",
    "\n",
    "    def radial_fun(self, batches):\n",
    "        n_input = batches.size(0) # number of inputs\n",
    "        A = self.centers.view(self.num_centers,-1).repeat(n_input,1,1)\n",
    "        B = batches.view(n_input,-1).unsqueeze(1).repeat(1,self.num_centers,1)\n",
    "        C = torch.exp(-self.beta.mul((A-B).pow(2).sum(2,keepdim=False).sqrt() ) )\n",
    "        return C\n",
    "    \n",
    "    def forward(self, batches):\n",
    "        radial_val = self.radial_fun(batches)\n",
    "        class_score = self.linear(radial_val)\n",
    "        return class_score\n",
    "\n",
    "\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "\n",
    "centers = batch_images\n",
    "#centers = torch.rand(1000,28*28)\n",
    "\n",
    "#model2 = RbfNet(Variable(centers.cuda()), num_class=10)\n",
    "model2 = RbfNet(centers, num_class=10)\n",
    " \n",
    "#model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "sgd = optim.SGD(model2.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.314688\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.293928\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.293468\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.275961\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.285246\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.286864\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.254691\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.281427\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.258626\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.253433\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.258692\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.256933\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.260344\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.256142\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.254433\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.230786\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.238441\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.227219\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.220328\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.214476\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.199323\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.153435\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.057044\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.076194\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.131581\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.064497\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.081757\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.115959\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.009063\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.155241\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.008608\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.078634\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.996955\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.038944\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.098219\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.037330\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.101323\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.053134\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.938641\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.085489\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.111212\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.000432\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.990628\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.964463\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.935674\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.980293\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.000947\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.069833\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.861007\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.963205\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.866299\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.689193\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.937043\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.839768\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.843531\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.888550\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.546959\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.797507\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.789355\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.716555\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.743201\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.957879\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.865868\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.630673\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.938740\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.776470\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.784151\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.652006\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.859623\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.681197\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.916239\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.572323\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.688776\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.821513\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.730397\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.670110\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.709556\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.954219\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.864000\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.665406\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.755171\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.922103\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.904267\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.993010\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.896029\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.689082\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.662343\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.650063\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.716074\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.590403\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.664486\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.632908\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.606748\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.747096\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.723616\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.863629\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.602642\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.736217\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.743496\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.681446\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        sgd.zero_grad()\n",
    "        output = model2(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([7, 2, 1, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "b_size = 5\n",
    "data = test_loader.dataset.data[0:b_size].data.unsqueeze(1).float()\n",
    "target = test_loader.dataset.targets[0:b_size]\n",
    "pred = model2(data)\n",
    "print(pred.argmax(dim=1, keepdim=False))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0269, Accuracy: 4459/10000 (44.6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "# pour ne pas calculer les gradients (gain de temps et de mémoire)\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model2(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
