{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # convertit les images [H,W,C] dont les valeurs sont dans [0,255] en tenseurs [C,H,W] dont les valeurs sont dans [0,1]\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "num_centers = 100\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "epsilon = 0.3\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# There we create the class for our Shallow RBF network\n",
    "\n",
    "class ShallowRBF(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_centers):\n",
    "        super(ShallowRBF, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_centers, input_dim))\n",
    "        self.beta = nn.Parameter(torch.ones(num_centers))\n",
    "        self.fc = nn.Linear(num_centers, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        # Calculate the RBF activations\n",
    "        rbf_activations = torch.exp(-self.beta * torch.norm(x.unsqueeze(1) - self.centers, dim=2))\n",
    "\n",
    "        # Normalize the RBF activations\n",
    "        rbf_activations = rbf_activations / torch.sum(rbf_activations, dim=1, keepdim=True)\n",
    "\n",
    "        # Pass the normalized RBF activations through the linear layer\n",
    "        output = self.fc(rbf_activations)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ShallowRBF(input_dim, num_classes, num_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "sgd = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.298555\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.300423\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.306037\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.297645\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.307513\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.305146\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.310460\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.311368\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.300928\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.303778\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304310\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.297438\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.290916\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.302901\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.306174\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.308681\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.296962\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.294380\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.304448\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.303810\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.294964\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.293807\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.297426\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.297634\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.299474\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.298382\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.302217\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.292063\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.288071\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.294354\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.301079\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.299434\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.297872\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.298124\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.299445\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.294416\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.290153\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.298057\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.282853\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.292660\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.295160\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.286233\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.297518\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.293012\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.278103\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.299421\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.283463\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.281593\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.290997\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.293538\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.305811\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.305276\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.291218\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.275503\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.301683\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.299663\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.291325\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.303133\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.292094\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.282147\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.288817\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.288686\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.288606\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.299568\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.274919\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.287362\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.286235\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.288730\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.284009\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.279859\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.270110\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.286541\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.289603\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.281101\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.289131\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.292690\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.281389\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.285193\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.288034\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.281100\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.276265\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.282092\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.292357\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.280318\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.270324\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.280610\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.286652\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.289572\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.288471\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.273124\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.292273\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.267754\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.270273\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.267008\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.277626\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.280042\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.274724\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.278477\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.281729\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.292751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        sgd.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0])\n",
      "tensor([7, 2, 1, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "b_size = 5\n",
    "data = test_loader.dataset.data[0:b_size].data.unsqueeze(1).float()\n",
    "target = test_loader.dataset.targets[0:b_size]\n",
    "pred = model(data)\n",
    "print(pred.argmax(dim=1, keepdim=False))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 1305/10000 (13.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "# pour ne pas calculer les gradients (gain de temps et de mémoire)\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
