{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1965315.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 2418215.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 822281.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 646087.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # convertit les images [H,W,C] dont les valeurs sont dans [0,255] en tenseurs [C,H,W] dont les valeurs sont dans [0,1]\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# There we create the class for our Shallow RBF network\n",
    "\n",
    "class ShallowRBF(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_centers):\n",
    "        super(ShallowRBF, self).__init__()\n",
    "        self.centers = nn.Parameter(torch.randn(num_centers, input_dim))\n",
    "        self.beta = nn.Parameter(torch.ones(num_centers))\n",
    "        self.fc = nn.Linear(num_centers, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the RBF activations\n",
    "        rbf_activations = torch.exp(-self.beta * torch.norm(x.unsqueeze(1) - self.centers, dim=2))\n",
    "\n",
    "        # Normalize the RBF activations\n",
    "        rbf_activations = rbf_activations / torch.sum(rbf_activations, dim=1, keepdim=True)\n",
    "\n",
    "        # Pass the normalized RBF activations through the linear layer\n",
    "        output = self.fc(rbf_activations)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ShallowRBF(input_dim, num_classes, num_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "input_dim = 784\n",
    "num_classes = 10\n",
    "num_centers = 100\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "epsilon = 0.3\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "sgd = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (784) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cellule 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m     sgd\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=5'>6</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=6'>7</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mUntitled-1.ipynb Cellule 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Calculate the RBF activations\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=11'>12</a>\u001b[0m     rbf_activations \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenters, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Normalize the RBF activations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=14'>15</a>\u001b[0m     rbf_activations \u001b[39m=\u001b[39m rbf_activations \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(rbf_activations, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (784) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        sgd.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
